<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Sheetaの摸鱼日记（四） | 等到天亮我们都寻找到最漂亮的愿望</title><meta name="keywords" content="diary"><meta name="author" content="Sheeta Liu"><meta name="copyright" content="Sheeta Liu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="忘掉种过的花，重新的出发放弃理想吧。">
<meta property="og:type" content="article">
<meta property="og:title" content="Sheetaの摸鱼日记（四）">
<meta property="og:url" content="http://liuydd.github.io/2023/02/19/moyu4/index.html">
<meta property="og:site_name" content="等到天亮我们都寻找到最漂亮的愿望">
<meta property="og:description" content="忘掉种过的花，重新的出发放弃理想吧。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://liuydd.github.io/images/cover33.jpg">
<meta property="article:published_time" content="2023-02-19T14:09:03.000Z">
<meta property="article:modified_time" content="2023-03-07T16:01:37.050Z">
<meta property="article:author" content="Sheeta Liu">
<meta property="article:tag" content="diary">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://liuydd.github.io/images/cover33.jpg"><link rel="shortcut icon" href="../../../../img/tubiao.jpg"><link rel="canonical" href="http://liuydd.github.io/2023/02/19/moyu4/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="../../../../css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Sheetaの摸鱼日记（四）',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-03-08 00:01:37'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="//at.alicdn.com/t/c/font_3870185_ugzi44wm6l.css"><meta name="generator" content="Hexo 6.2.0"><link rel="alternate" href="atom.xml" title="等到天亮我们都寻找到最漂亮的愿望" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="../img/touxiang2.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="../archives/"><div class="headline">文章</div><div class="length-num">18</div></a><a href="../tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/"><div class="headline">分类</div><div class="length-num">8</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="../index.html"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="../archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="../categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="../link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="../about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page" href="../html/Site/Resume.html"><i class="fa-fw icon-shanguangdengdakai"></i><span> Resume</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('../../../../images/cover33.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="../index.html">等到天亮我们都寻找到最漂亮的愿望</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="../index.html"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="../archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="../categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="../link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="../about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div><div class="menus_item"><a class="site-page" href="../html/Site/Resume.html"><i class="fa-fw icon-shanguangdengdakai"></i><span> Resume</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Sheetaの摸鱼日记（四）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-02-19T14:09:03.000Z" title="发表于 2023-02-19 22:09:03">2023-02-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-03-07T16:01:37.050Z" title="更新于 2023-03-08 00:01:37">2023-03-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="../../../../categories/%E6%B5%AE%E5%85%89%E6%8E%A0%E5%BD%B1/">浮光掠影</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">8.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>34分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Sheetaの摸鱼日记（四）"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><div class="note info modern"><p>我已经分不清我现在是到底需要一个拥抱、喝十瓶碳酸饮料、看心理医生、一觉睡六个月、还是被车撞死。</p>
</div>

<h1 id="2023-x2F-2-x2F-20"><a href="#2023-x2F-2-x2F-20" class="headerlink" title="2023&#x2F;2&#x2F;20"></a>2023&#x2F;2&#x2F;20</h1><p>开学第一天，有点想家了。想想已经离开家二十多天了，想爸妈，想我的朋友，想家那边的好吃的比如矮子馅饼。</p>
<p>I’m so far away from home.</p>
<p>第一天就感觉有点进不去状态，似乎丧失了学习的热情与能力。</p>
<p>算了，慢慢来吧，感觉绩点对我来说已经没那么重要了，尽管我想出国留学。</p>
<p>想在新的学期尝试一些新的事情。</p>
<p>——————</p>
<p>记录一下environment myenv:</p>
<p>Python: 3.8.0</p>
<p>pip: 23.0</p>
<p>torch: 1.10.1</p>
<p>transformers: 4.26.1</p>
<p>cuda: 10.2</p>
<p>——————</p>
<p>nvcc -V:</p>
<p>nvcc: NVIDIA (R) Cuda compiler driver<br>Copyright (c) 2005-2022 NVIDIA Corporation<br>Built on Tue_Mar__8_18:18:20_PST_2022<br>Cuda compilation tools, release 11.6, V11.6.124<br>Build cuda_11.6.r11.6&#x2F;compiler.31057947_0</p>
<p>——————</p>
<p><strong>创建虚拟环境并指定PYTHON版本</strong>:</p>
<p><code>virtualenv venv --python=python2.7</code></p>
<p>分配一块GPU：</p>
<p><code>srun -G 1--pty --nodelist=99server --mem 4g bash</code></p>
<p>.</p>
<h1 id="2023-x2F-2-x2F-21"><a href="#2023-x2F-2-x2F-21" class="headerlink" title="2023&#x2F;2&#x2F;21"></a>2023&#x2F;2&#x2F;21</h1><p>我永远讨厌物理实验🥀</p>
<p>——————————</p>
<p>值得纪念：</p>
<p><img src="/2023/02/19/moyu4/1.jpg"></p>
<p>突然间，配了快一周的环境能运行之前运行不了的一段代码了…尽管配环境过程中出现的bug我还没有解决…</p>
<p>暂时不管了，代码能跑就不要再改了（）</p>
<p>开始任务1（终于），不过还没有掌握slurm的用法以及不知道怎么用GPU🥀</p>
<p>顺便说一句，在配环境的过程中遇到奇奇怪怪的报错让我觉得“编译compile”真是奇妙。</p>
<p>————————</p>
<p>记录一下目前进度：</p>
<p>fairseq到了（2）Train a language model，输入官方文档中的代码时出现报错，输出为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line">2023-02-21 21:04:52 | INFO | fairseq_cli.train | &#123;<span class="string">&#x27;_name&#x27;</span>: None, <span class="string">&#x27;common&#x27;</span>: &#123;<span class="string">&#x27;_name&#x27;</span>: None, <span class="string">&#x27;no_progress_bar&#x27;</span>: False, <span class="string">&#x27;log_interval&#x27;</span>: 100, <span class="string">&#x27;log_format&#x27;</span>: None, <span class="string">&#x27;log_file&#x27;</span>: None, <span class="string">&#x27;aim_repo&#x27;</span>: None, <span class="string">&#x27;aim_run_hash&#x27;</span>: None, <span class="string">&#x27;tensorboard_logdir&#x27;</span>: None, <span class="string">&#x27;wandb_project&#x27;</span>: None, <span class="string">&#x27;azureml_logging&#x27;</span>: False, <span class="string">&#x27;seed&#x27;</span>: 1, <span class="string">&#x27;cpu&#x27;</span>: False, <span class="string">&#x27;tpu&#x27;</span>: False, <span class="string">&#x27;bf16&#x27;</span>: False, <span class="string">&#x27;memory_efficient_bf16&#x27;</span>: False, <span class="string">&#x27;fp16&#x27;</span>: True, <span class="string">&#x27;memory_efficient_fp16&#x27;</span>: False, <span class="string">&#x27;fp16_no_flatten_grads&#x27;</span>: False, <span class="string">&#x27;fp16_init_scale&#x27;</span>: 128, <span class="string">&#x27;fp16_scale_window&#x27;</span>: None, <span class="string">&#x27;fp16_scale_tolerance&#x27;</span>: 0.0, <span class="string">&#x27;on_cpu_convert_precision&#x27;</span>: False, <span class="string">&#x27;min_loss_scale&#x27;</span>: 0.0001, <span class="string">&#x27;threshold_loss_scale&#x27;</span>: None, <span class="string">&#x27;amp&#x27;</span>: False, <span class="string">&#x27;amp_batch_retries&#x27;</span>: 2, <span class="string">&#x27;amp_init_scale&#x27;</span>: 128, <span class="string">&#x27;amp_scale_window&#x27;</span>: None, <span class="string">&#x27;user_dir&#x27;</span>: None, <span class="string">&#x27;empty_cache_freq&#x27;</span>: 0, <span class="string">&#x27;all_gather_list_size&#x27;</span>: 16384, <span class="string">&#x27;model_parallel_size&#x27;</span>: 1, <span class="string">&#x27;quantization_config_path&#x27;</span>: None, <span class="string">&#x27;profile&#x27;</span>: False, <span class="string">&#x27;reset_logging&#x27;</span>: False, <span class="string">&#x27;suppress_crashes&#x27;</span>: False, <span class="string">&#x27;use_plasma_view&#x27;</span>: False, <span class="string">&#x27;plasma_path&#x27;</span>: <span class="string">&#x27;/tmp/plasma&#x27;</span>&#125;, <span class="string">&#x27;common_eval&#x27;</span>: &#123;<span class="string">&#x27;_name&#x27;</span>: None, <span class="string">&#x27;path&#x27;</span>: None, <span class="string">&#x27;post_process&#x27;</span>: None, <span class="string">&#x27;quiet&#x27;</span>: False, <span class="string">&#x27;model_overrides&#x27;</span>: <span class="string">&#x27;&#123;&#125;&#x27;</span>, <span class="string">&#x27;results_path&#x27;</span>: None&#125;, <span class="string">&#x27;distributed_training&#x27;</span>: &#123;<span class="string">&#x27;_name&#x27;</span>: None, <span class="string">&#x27;distributed_world_size&#x27;</span>: 1, <span class="string">&#x27;distributed_num_procs&#x27;</span>: 0, <span class="string">&#x27;distributed_rank&#x27;</span>: 0, <span class="string">&#x27;distributed_backend&#x27;</span>: <span class="string">&#x27;nccl&#x27;</span>, <span class="string">&#x27;distributed_init_method&#x27;</span>: None, <span class="string">&#x27;distributed_port&#x27;</span>: -1, <span class="string">&#x27;device_id&#x27;</span>: 0, <span class="string">&#x27;distributed_no_spawn&#x27;</span>: False, <span class="string">&#x27;ddp_backend&#x27;</span>: <span class="string">&#x27;pytorch_ddp&#x27;</span>, <span class="string">&#x27;ddp_comm_hook&#x27;</span>: <span class="string">&#x27;none&#x27;</span>, <span class="string">&#x27;bucket_cap_mb&#x27;</span>: 25, <span class="string">&#x27;fix_batches_to_gpus&#x27;</span>: False, <span class="string">&#x27;find_unused_parameters&#x27;</span>: False, <span class="string">&#x27;gradient_as_bucket_view&#x27;</span>: False, <span class="string">&#x27;fast_stat_sync&#x27;</span>: False, <span class="string">&#x27;heartbeat_timeout&#x27;</span>: -1, <span class="string">&#x27;broadcast_buffers&#x27;</span>: False, <span class="string">&#x27;slowmo_momentum&#x27;</span>: None, <span class="string">&#x27;slowmo_base_algorithm&#x27;</span>: <span class="string">&#x27;localsgd&#x27;</span>, <span class="string">&#x27;localsgd_frequency&#x27;</span>: 3, <span class="string">&#x27;nprocs_per_node&#x27;</span>: 1, <span class="string">&#x27;pipeline_model_parallel&#x27;</span>: False, <span class="string">&#x27;pipeline_balance&#x27;</span>: None, <span class="string">&#x27;pipeline_devices&#x27;</span>: None, <span class="string">&#x27;pipeline_chunks&#x27;</span>: 0, <span class="string">&#x27;pipeline_encoder_balance&#x27;</span>: None, <span class="string">&#x27;pipeline_encoder_devices&#x27;</span>: None, <span class="string">&#x27;pipeline_decoder_balance&#x27;</span>: None, <span class="string">&#x27;pipeline_decoder_devices&#x27;</span>: None, <span class="string">&#x27;pipeline_checkpoint&#x27;</span>: <span class="string">&#x27;never&#x27;</span>, <span class="string">&#x27;zero_sharding&#x27;</span>: <span class="string">&#x27;none&#x27;</span>, <span class="string">&#x27;fp16&#x27;</span>: True, <span class="string">&#x27;memory_efficient_fp16&#x27;</span>: False, <span class="string">&#x27;tpu&#x27;</span>: False, <span class="string">&#x27;no_reshard_after_forward&#x27;</span>: False, <span class="string">&#x27;fp32_reduce_scatter&#x27;</span>: False, <span class="string">&#x27;cpu_offload&#x27;</span>: False, <span class="string">&#x27;use_sharded_state&#x27;</span>: False, <span class="string">&#x27;not_fsdp_flatten_parameters&#x27;</span>: False&#125;, <span class="string">&#x27;dataset&#x27;</span>: &#123;<span class="string">&#x27;_name&#x27;</span>: None, <span class="string">&#x27;num_workers&#x27;</span>: 1, <span class="string">&#x27;skip_invalid_size_inputs_valid_test&#x27;</span>: False, <span class="string">&#x27;max_tokens&#x27;</span>: 2048, <span class="string">&#x27;batch_size&#x27;</span>: None, <span class="string">&#x27;required_batch_size_multiple&#x27;</span>: 8, <span class="string">&#x27;required_seq_len_multiple&#x27;</span>: 1, <span class="string">&#x27;dataset_impl&#x27;</span>: None, <span class="string">&#x27;data_buffer_size&#x27;</span>: 10, <span class="string">&#x27;train_subset&#x27;</span>: <span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;valid_subset&#x27;</span>: <span class="string">&#x27;valid&#x27;</span>, <span class="string">&#x27;combine_valid_subsets&#x27;</span>: None, <span class="string">&#x27;ignore_unused_valid_subsets&#x27;</span>: False, <span class="string">&#x27;validate_interval&#x27;</span>: 1, <span class="string">&#x27;validate_interval_updates&#x27;</span>: 0, <span class="string">&#x27;validate_after_updates&#x27;</span>: 0, <span class="string">&#x27;fixed_validation_seed&#x27;</span>: None, <span class="string">&#x27;disable_validation&#x27;</span>: False, <span class="string">&#x27;max_tokens_valid&#x27;</span>: 2048, <span class="string">&#x27;batch_size_valid&#x27;</span>: None, <span class="string">&#x27;max_valid_steps&#x27;</span>: None, <span class="string">&#x27;curriculum&#x27;</span>: 0, <span class="string">&#x27;gen_subset&#x27;</span>: <span class="string">&#x27;test&#x27;</span>, <span class="string">&#x27;num_shards&#x27;</span>: 1, <span class="string">&#x27;shard_id&#x27;</span>: 0, <span class="string">&#x27;grouped_shuffling&#x27;</span>: False, <span class="string">&#x27;update_epoch_batch_itr&#x27;</span>: False, <span class="string">&#x27;update_ordered_indices_seed&#x27;</span>: False&#125;, <span class="string">&#x27;optimization&#x27;</span>: &#123;<span class="string">&#x27;_name&#x27;</span>: None, <span class="string">&#x27;max_epoch&#x27;</span>: 0, <span class="string">&#x27;max_update&#x27;</span>: 50000, <span class="string">&#x27;stop_time_hours&#x27;</span>: 0.0, <span class="string">&#x27;clip_norm&#x27;</span>: 0.0, <span class="string">&#x27;sentence_avg&#x27;</span>: False, <span class="string">&#x27;update_freq&#x27;</span>: [16], <span class="string">&#x27;lr&#x27;</span>: [0.0005], <span class="string">&#x27;stop_min_lr&#x27;</span>: -1.0, <span class="string">&#x27;use_bmuf&#x27;</span>: False, <span class="string">&#x27;skip_remainder_batch&#x27;</span>: False&#125;, <span class="string">&#x27;checkpoint&#x27;</span>: &#123;<span class="string">&#x27;_name&#x27;</span>: None, <span class="string">&#x27;save_dir&#x27;</span>: <span class="string">&#x27;checkpoints/transformer_wikitext-103&#x27;</span>, <span class="string">&#x27;restore_file&#x27;</span>: <span class="string">&#x27;checkpoint_last.pt&#x27;</span>, <span class="string">&#x27;continue_once&#x27;</span>: None, <span class="string">&#x27;finetune_from_model&#x27;</span>: None, <span class="string">&#x27;reset_dataloader&#x27;</span>: False, <span class="string">&#x27;reset_lr_scheduler&#x27;</span>: False, <span class="string">&#x27;reset_meters&#x27;</span>: False, <span class="string">&#x27;reset_optimizer&#x27;</span>: False, <span class="string">&#x27;optimizer_overrides&#x27;</span>: <span class="string">&#x27;&#123;&#125;&#x27;</span>, <span class="string">&#x27;save_interval&#x27;</span>: 1, <span class="string">&#x27;save_interval_updates&#x27;</span>: 0, <span class="string">&#x27;keep_interval_updates&#x27;</span>: -1, <span class="string">&#x27;keep_interval_updates_pattern&#x27;</span>: -1, <span class="string">&#x27;keep_last_epochs&#x27;</span>: -1, <span class="string">&#x27;keep_best_checkpoints&#x27;</span>: -1, <span class="string">&#x27;no_save&#x27;</span>: False, <span class="string">&#x27;no_epoch_checkpoints&#x27;</span>: False, <span class="string">&#x27;no_last_checkpoints&#x27;</span>: False, <span class="string">&#x27;no_save_optimizer_state&#x27;</span>: False, <span class="string">&#x27;best_checkpoint_metric&#x27;</span>: <span class="string">&#x27;loss&#x27;</span>, <span class="string">&#x27;maximize_best_checkpoint_metric&#x27;</span>: False, <span class="string">&#x27;patience&#x27;</span>: -1, <span class="string">&#x27;checkpoint_suffix&#x27;</span>: <span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;checkpoint_shard_count&#x27;</span>: 1, <span class="string">&#x27;load_checkpoint_on_all_dp_ranks&#x27;</span>: False, <span class="string">&#x27;write_checkpoints_asynchronously&#x27;</span>: False, <span class="string">&#x27;model_parallel_size&#x27;</span>: 1&#125;, <span class="string">&#x27;bmuf&#x27;</span>: &#123;<span class="string">&#x27;_name&#x27;</span>: None, <span class="string">&#x27;block_lr&#x27;</span>: 1.0, <span class="string">&#x27;block_momentum&#x27;</span>: 0.875, <span class="string">&#x27;global_sync_iter&#x27;</span>: 50, <span class="string">&#x27;warmup_iterations&#x27;</span>: 500, <span class="string">&#x27;use_nbm&#x27;</span>: False, <span class="string">&#x27;average_sync&#x27;</span>: False, <span class="string">&#x27;distributed_world_size&#x27;</span>: 1&#125;, <span class="string">&#x27;generation&#x27;</span>: &#123;<span class="string">&#x27;_name&#x27;</span>: None, <span class="string">&#x27;beam&#x27;</span>: 5, <span class="string">&#x27;nbest&#x27;</span>: 1, <span class="string">&#x27;max_len_a&#x27;</span>: 0.0, <span class="string">&#x27;max_len_b&#x27;</span>: 200, <span class="string">&#x27;min_len&#x27;</span>: 1, <span class="string">&#x27;match_source_len&#x27;</span>: False, <span class="string">&#x27;unnormalized&#x27;</span>: False, <span class="string">&#x27;no_early_stop&#x27;</span>: False, <span class="string">&#x27;no_beamable_mm&#x27;</span>: False, <span class="string">&#x27;lenpen&#x27;</span>: 1.0, <span class="string">&#x27;unkpen&#x27;</span>: 0.0, <span class="string">&#x27;replace_unk&#x27;</span>: None, <span class="string">&#x27;sacrebleu&#x27;</span>: False, <span class="string">&#x27;score_reference&#x27;</span>: False, <span class="string">&#x27;prefix_size&#x27;</span>: 0, <span class="string">&#x27;no_repeat_ngram_size&#x27;</span>: 0, <span class="string">&#x27;sampling&#x27;</span>: False, <span class="string">&#x27;sampling_topk&#x27;</span>: -1, <span class="string">&#x27;sampling_topp&#x27;</span>: -1.0, <span class="string">&#x27;constraints&#x27;</span>: None, <span class="string">&#x27;temperature&#x27;</span>: 1.0, <span class="string">&#x27;diverse_beam_groups&#x27;</span>: -1, <span class="string">&#x27;diverse_beam_strength&#x27;</span>: 0.5, <span class="string">&#x27;diversity_rate&#x27;</span>: -1.0, <span class="string">&#x27;print_alignment&#x27;</span>: None, <span class="string">&#x27;print_step&#x27;</span>: False, <span class="string">&#x27;lm_path&#x27;</span>: None, <span class="string">&#x27;lm_weight&#x27;</span>: 0.0, <span class="string">&#x27;iter_decode_eos_penalty&#x27;</span>: 0.0, <span class="string">&#x27;iter_decode_max_iter&#x27;</span>: 10, <span class="string">&#x27;iter_decode_force_max_iter&#x27;</span>: False, <span class="string">&#x27;iter_decode_with_beam&#x27;</span>: 1, <span class="string">&#x27;iter_decode_with_external_reranker&#x27;</span>: False, <span class="string">&#x27;retain_iter_history&#x27;</span>: False, <span class="string">&#x27;retain_dropout&#x27;</span>: False, <span class="string">&#x27;retain_dropout_modules&#x27;</span>: None, <span class="string">&#x27;decoding_format&#x27;</span>: None, <span class="string">&#x27;no_seed_provided&#x27;</span>: False, <span class="string">&#x27;eos_token&#x27;</span>: None&#125;, <span class="string">&#x27;eval_lm&#x27;</span>: &#123;<span class="string">&#x27;_name&#x27;</span>: None, <span class="string">&#x27;output_word_probs&#x27;</span>: False, <span class="string">&#x27;output_word_stats&#x27;</span>: False, <span class="string">&#x27;context_window&#x27;</span>: 0, <span class="string">&#x27;softmax_batch&#x27;</span>: 9223372036854775807&#125;, <span class="string">&#x27;interactive&#x27;</span>: &#123;<span class="string">&#x27;_name&#x27;</span>: None, <span class="string">&#x27;buffer_size&#x27;</span>: 0, <span class="string">&#x27;input&#x27;</span>: <span class="string">&#x27;-&#x27;</span>&#125;, <span class="string">&#x27;model&#x27;</span>: &#123;<span class="string">&#x27;_name&#x27;</span>: <span class="string">&#x27;transformer_lm&#x27;</span>, <span class="string">&#x27;activation_fn&#x27;</span>: relu, <span class="string">&#x27;dropout&#x27;</span>: 0.1, <span class="string">&#x27;attention_dropout&#x27;</span>: 0.0, <span class="string">&#x27;activation_dropout&#x27;</span>: 0.0, <span class="string">&#x27;relu_dropout&#x27;</span>: 0.0, <span class="string">&#x27;decoder_embed_dim&#x27;</span>: 512, <span class="string">&#x27;decoder_output_dim&#x27;</span>: 512, <span class="string">&#x27;decoder_input_dim&#x27;</span>: 512, <span class="string">&#x27;decoder_ffn_embed_dim&#x27;</span>: 2048, <span class="string">&#x27;decoder_layers&#x27;</span>: 6, <span class="string">&#x27;decoder_attention_heads&#x27;</span>: 8, <span class="string">&#x27;decoder_normalize_before&#x27;</span>: False, <span class="string">&#x27;no_decoder_final_norm&#x27;</span>: False, <span class="string">&#x27;adaptive_softmax_cutoff&#x27;</span>: None, <span class="string">&#x27;adaptive_softmax_dropout&#x27;</span>: 0.0, <span class="string">&#x27;adaptive_softmax_factor&#x27;</span>: 4.0, <span class="string">&#x27;no_token_positional_embeddings&#x27;</span>: False, <span class="string">&#x27;share_decoder_input_output_embed&#x27;</span>: True, <span class="string">&#x27;character_embeddings&#x27;</span>: False, <span class="string">&#x27;character_filters&#x27;</span>: <span class="string">&#x27;[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]&#x27;</span>, <span class="string">&#x27;character_embedding_dim&#x27;</span>: 4, <span class="string">&#x27;char_embedder_highway_layers&#x27;</span>: 2, <span class="string">&#x27;adaptive_input&#x27;</span>: False, <span class="string">&#x27;adaptive_input_factor&#x27;</span>: 4.0, <span class="string">&#x27;adaptive_input_cutoff&#x27;</span>: None, <span class="string">&#x27;tie_adaptive_weights&#x27;</span>: False, <span class="string">&#x27;tie_adaptive_proj&#x27;</span>: False, <span class="string">&#x27;decoder_learned_pos&#x27;</span>: False, <span class="string">&#x27;layernorm_embedding&#x27;</span>: False, <span class="string">&#x27;no_scale_embedding&#x27;</span>: False, <span class="string">&#x27;checkpoint_activations&#x27;</span>: False, <span class="string">&#x27;offload_activations&#x27;</span>: False, <span class="string">&#x27;decoder_layerdrop&#x27;</span>: 0.0, <span class="string">&#x27;decoder_layers_to_keep&#x27;</span>: None, <span class="string">&#x27;quant_noise_pq&#x27;</span>: 0.0, <span class="string">&#x27;quant_noise_pq_block_size&#x27;</span>: 8, <span class="string">&#x27;quant_noise_scalar&#x27;</span>: 0.0, <span class="string">&#x27;min_params_to_wrap&#x27;</span>: 100000000, <span class="string">&#x27;base_layers&#x27;</span>: 0, <span class="string">&#x27;base_sublayers&#x27;</span>: 1, <span class="string">&#x27;base_shuffle&#x27;</span>: 1, <span class="string">&#x27;scale_fc&#x27;</span>: False, <span class="string">&#x27;scale_attn&#x27;</span>: False, <span class="string">&#x27;scale_heads&#x27;</span>: False, <span class="string">&#x27;scale_resids&#x27;</span>: False, <span class="string">&#x27;decoder_xformers_att_config&#x27;</span>: None, <span class="string">&#x27;add_bos_token&#x27;</span>: False, <span class="string">&#x27;tokens_per_sample&#x27;</span>: 512, <span class="string">&#x27;max_target_positions&#x27;</span>: None, <span class="string">&#x27;tpu&#x27;</span>: False&#125;, <span class="string">&#x27;task&#x27;</span>: &#123;<span class="string">&#x27;_name&#x27;</span>: <span class="string">&#x27;language_modeling&#x27;</span>, <span class="string">&#x27;data&#x27;</span>: <span class="string">&#x27;data-bin/wikitext-103&#x27;</span>, <span class="string">&#x27;sample_break_mode&#x27;</span>: none, <span class="string">&#x27;tokens_per_sample&#x27;</span>: 512, <span class="string">&#x27;output_dictionary_size&#x27;</span>: -1, <span class="string">&#x27;self_target&#x27;</span>: False, <span class="string">&#x27;future_target&#x27;</span>: False, <span class="string">&#x27;past_target&#x27;</span>: False, <span class="string">&#x27;add_bos_token&#x27;</span>: False, <span class="string">&#x27;max_target_positions&#x27;</span>: None, <span class="string">&#x27;shorten_method&#x27;</span>: none, <span class="string">&#x27;shorten_data_split_list&#x27;</span>: <span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;pad_to_fixed_length&#x27;</span>: False, <span class="string">&#x27;pad_to_fixed_bsz&#x27;</span>: False, <span class="string">&#x27;seed&#x27;</span>: 1, <span class="string">&#x27;batch_size&#x27;</span>: None, <span class="string">&#x27;batch_size_valid&#x27;</span>: None, <span class="string">&#x27;dataset_impl&#x27;</span>: None, <span class="string">&#x27;data_buffer_size&#x27;</span>: 10, <span class="string">&#x27;tpu&#x27;</span>: False, <span class="string">&#x27;use_plasma_view&#x27;</span>: False, <span class="string">&#x27;plasma_path&#x27;</span>: <span class="string">&#x27;/tmp/plasma&#x27;</span>&#125;, <span class="string">&#x27;criterion&#x27;</span>: &#123;<span class="string">&#x27;_name&#x27;</span>: <span class="string">&#x27;cross_entropy&#x27;</span>, <span class="string">&#x27;sentence_avg&#x27;</span>: False&#125;, <span class="string">&#x27;optimizer&#x27;</span>: &#123;<span class="string">&#x27;_name&#x27;</span>: <span class="string">&#x27;adam&#x27;</span>, <span class="string">&#x27;adam_betas&#x27;</span>: <span class="string">&#x27;(0.9, 0.98)&#x27;</span>, <span class="string">&#x27;adam_eps&#x27;</span>: 1e-08, <span class="string">&#x27;weight_decay&#x27;</span>: 0.01, <span class="string">&#x27;use_old_adam&#x27;</span>: False, <span class="string">&#x27;fp16_adam_stats&#x27;</span>: False, <span class="string">&#x27;tpu&#x27;</span>: False, <span class="string">&#x27;lr&#x27;</span>: [0.0005]&#125;, <span class="string">&#x27;lr_scheduler&#x27;</span>: &#123;<span class="string">&#x27;_name&#x27;</span>: <span class="string">&#x27;inverse_sqrt&#x27;</span>, <span class="string">&#x27;warmup_updates&#x27;</span>: 4000, <span class="string">&#x27;warmup_init_lr&#x27;</span>: 1e-07, <span class="string">&#x27;lr&#x27;</span>: [0.0005]&#125;, <span class="string">&#x27;scoring&#x27;</span>: &#123;<span class="string">&#x27;_name&#x27;</span>: <span class="string">&#x27;bleu&#x27;</span>, <span class="string">&#x27;pad&#x27;</span>: 1, <span class="string">&#x27;eos&#x27;</span>: 2, <span class="string">&#x27;unk&#x27;</span>: 3&#125;, <span class="string">&#x27;bpe&#x27;</span>: None, <span class="string">&#x27;tokenizer&#x27;</span>: None, <span class="string">&#x27;ema&#x27;</span>: &#123;<span class="string">&#x27;_name&#x27;</span>: None, <span class="string">&#x27;store_ema&#x27;</span>: False, <span class="string">&#x27;ema_decay&#x27;</span>: 0.9999, <span class="string">&#x27;ema_start_update&#x27;</span>: 0, <span class="string">&#x27;ema_seed_model&#x27;</span>: None, <span class="string">&#x27;ema_update_freq&#x27;</span>: 1, <span class="string">&#x27;ema_fp32&#x27;</span>: False&#125;&#125;</span><br><span class="line">2023-02-21 21:04:52 | INFO | fairseq.tasks.language_modeling | dictionary: 267744 types</span><br><span class="line">2023-02-21 21:04:55 | INFO | fairseq_cli.train | TransformerLanguageModel(</span><br><span class="line">  (decoder): TransformerDecoder(</span><br><span class="line">    (dropout_module): FairseqDropout()</span><br><span class="line">    (embed_tokens): Embedding(267744, 512, padding_idx=1)</span><br><span class="line">    (embed_positions): SinusoidalPositionalEmbedding()</span><br><span class="line">    (layers): ModuleList(</span><br><span class="line">      (0): TransformerDecoderLayerBase(</span><br><span class="line">        (dropout_module): FairseqDropout()</span><br><span class="line">        (self_attn): MultiheadAttention(</span><br><span class="line">          (dropout_module): FairseqDropout()</span><br><span class="line">          (k_proj): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          (v_proj): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          (q_proj): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          (out_proj): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">        )</span><br><span class="line">        (activation_dropout_module): FairseqDropout()</span><br><span class="line">        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)</span><br><span class="line">        (fc1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">        (fc2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)</span><br><span class="line">      )</span><br><span class="line">      (1): TransformerDecoderLayerBase(</span><br><span class="line">        (dropout_module): FairseqDropout()</span><br><span class="line">        (self_attn): MultiheadAttention(</span><br><span class="line">          (dropout_module): FairseqDropout()</span><br><span class="line">          (k_proj): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          (v_proj): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          (q_proj): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          (out_proj): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">        )</span><br><span class="line">        (activation_dropout_module): FairseqDropout()</span><br><span class="line">        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)</span><br><span class="line">        (fc1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">        (fc2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)</span><br><span class="line">      )</span><br><span class="line">      (2): TransformerDecoderLayerBase(</span><br><span class="line">        (dropout_module): FairseqDropout()</span><br><span class="line">        (self_attn): MultiheadAttention(</span><br><span class="line">          (dropout_module): FairseqDropout()</span><br><span class="line">          (k_proj): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          (v_proj): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          (q_proj): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          (out_proj): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">        )</span><br><span class="line">        (activation_dropout_module): FairseqDropout()</span><br><span class="line">        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)</span><br><span class="line">        (fc1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">        (fc2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)</span><br><span class="line">      )</span><br><span class="line">      (3): TransformerDecoderLayerBase(</span><br><span class="line">        (dropout_module): FairseqDropout()</span><br><span class="line">        (self_attn): MultiheadAttention(</span><br><span class="line">          (dropout_module): FairseqDropout()</span><br><span class="line">          (k_proj): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          (v_proj): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          (q_proj): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          (out_proj): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">        )</span><br><span class="line">        (activation_dropout_module): FairseqDropout()</span><br><span class="line">        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)</span><br><span class="line">        (fc1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">        (fc2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)</span><br><span class="line">      )</span><br><span class="line">      (4): TransformerDecoderLayerBase(</span><br><span class="line">        (dropout_module): FairseqDropout()</span><br><span class="line">        (self_attn): MultiheadAttention(</span><br><span class="line">          (dropout_module): FairseqDropout()</span><br><span class="line">          (k_proj): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          (v_proj): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          (q_proj): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          (out_proj): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">        )</span><br><span class="line">        (activation_dropout_module): FairseqDropout()</span><br><span class="line">        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)</span><br><span class="line">        (fc1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">        (fc2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)</span><br><span class="line">      )</span><br><span class="line">      (5): TransformerDecoderLayerBase(</span><br><span class="line">        (dropout_module): FairseqDropout()</span><br><span class="line">        (self_attn): MultiheadAttention(</span><br><span class="line">          (dropout_module): FairseqDropout()</span><br><span class="line">          (k_proj): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          (v_proj): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          (q_proj): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          (out_proj): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">        )</span><br><span class="line">        (activation_dropout_module): FairseqDropout()</span><br><span class="line">        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)</span><br><span class="line">        (fc1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">        (fc2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (output_projection): Linear(in_features=512, out_features=267744, bias=False)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line">2023-02-21 21:04:55 | INFO | fairseq_cli.train | task: LanguageModelingTask</span><br><span class="line">2023-02-21 21:04:55 | INFO | fairseq_cli.train | model: TransformerLanguageModel</span><br><span class="line">2023-02-21 21:04:55 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion</span><br><span class="line">2023-02-21 21:04:55 | INFO | fairseq_cli.train | num. shared model params: 155,999,232 (num. trained: 155,999,232)</span><br><span class="line">2023-02-21 21:04:55 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)</span><br><span class="line">2023-02-21 21:04:55 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103/valid</span><br><span class="line">2023-02-21 21:04:55 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight &lt;- decoder.output_projection.weight</span><br><span class="line">2023-02-21 21:04:55 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)</span><br><span class="line">2023-02-21 21:04:55 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None</span><br><span class="line">2023-02-21 21:04:55 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/transformer_wikitext-103/checkpoint_last.pt</span><br><span class="line">2023-02-21 21:04:55 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/transformer_wikitext-103/checkpoint_last.pt</span><br><span class="line">2023-02-21 21:04:55 | INFO | fairseq.trainer | loading train data <span class="keyword">for</span> epoch 1</span><br><span class="line">2023-02-21 21:04:56 | INFO | fairseq.data.data_utils | loaded 1,801,350 examples from: data-bin/wikitext-103/train</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;/data_new/private/liuyadi/myenv/bin/fairseq-train&quot;</span>, line 8, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    sys.exit(cli_main())</span><br><span class="line">  File <span class="string">&quot;/data_new/private/liuyadi/myenv/lib/python3.8/site-packages/fairseq_cli/train.py&quot;</span>, line 557, <span class="keyword">in</span> cli_main</span><br><span class="line">    distributed_utils.call_main(cfg, main)</span><br><span class="line">  File <span class="string">&quot;/data_new/private/liuyadi/myenv/lib/python3.8/site-packages/fairseq/distributed/utils.py&quot;</span>, line 369, <span class="keyword">in</span> call_main</span><br><span class="line">    main(cfg, **kwargs)</span><br><span class="line">  File <span class="string">&quot;/data_new/private/liuyadi/myenv/lib/python3.8/site-packages/fairseq_cli/train.py&quot;</span>, line 164, <span class="keyword">in</span> main</span><br><span class="line">    extra_state, epoch_itr = checkpoint_utils.load_checkpoint(</span><br><span class="line">  File <span class="string">&quot;/data_new/private/liuyadi/myenv/lib/python3.8/site-packages/fairseq/checkpoint_utils.py&quot;</span>, line 276, <span class="keyword">in</span> load_checkpoint</span><br><span class="line">    trainer.lr_step(epoch_itr.epoch)</span><br><span class="line">  File <span class="string">&quot;/data_new/private/liuyadi/myenv/lib/python3.8/site-packages/fairseq/trainer.py&quot;</span>, line 1172, <span class="keyword">in</span> lr_step</span><br><span class="line">    self.lr_scheduler.step(epoch, val_loss)</span><br><span class="line">  File <span class="string">&quot;/data_new/private/liuyadi/myenv/lib/python3.8/site-packages/fairseq/trainer.py&quot;</span>, line 287, <span class="keyword">in</span> lr_scheduler</span><br><span class="line">    self._build_optimizer()  <span class="comment"># this will initialize self._lr_scheduler</span></span><br><span class="line">  File <span class="string">&quot;/data_new/private/liuyadi/myenv/lib/python3.8/site-packages/fairseq/trainer.py&quot;</span>, line 324, <span class="keyword">in</span> _build_optimizer</span><br><span class="line">    self._optimizer = optim.FP16Optimizer.build_optimizer(self.cfg, params)</span><br><span class="line">  File <span class="string">&quot;/data_new/private/liuyadi/myenv/lib/python3.8/site-packages/fairseq/optim/fp16_optimizer.py&quot;</span>, line 296, <span class="keyword">in</span> build_optimizer</span><br><span class="line">    fp32_params = cls.build_fp32_params(cfg.optimizer, params, flatten=flatten)</span><br><span class="line">  File <span class="string">&quot;/data_new/private/liuyadi/myenv/lib/python3.8/site-packages/fairseq/optim/fp16_optimizer.py&quot;</span>, line 38, <span class="keyword">in</span> build_fp32_params</span><br><span class="line">    devices = [torch.cuda.current_device()]</span><br><span class="line">  File <span class="string">&quot;/data_new/private/liuyadi/myenv/lib/python3.8/site-packages/torch/cuda/__init__.py&quot;</span>, line 479, <span class="keyword">in</span> current_device</span><br><span class="line">    _lazy_init()</span><br><span class="line">  File <span class="string">&quot;/data_new/private/liuyadi/myenv/lib/python3.8/site-packages/torch/cuda/__init__.py&quot;</span>, line 214, <span class="keyword">in</span> _lazy_init</span><br><span class="line">    torch._C._cuda_init()</span><br><span class="line">RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx</span><br></pre></td></tr></table></figure>

<p>怀疑是没有使用slurm命令使用GPU的原因，接下来需要实践简单看完的SLURM简明过程并且解决此问题，然后就可以开始调试模型了。</p>
<p>——————————</p>
<p>晚上骑车回宿舍的路上戴耳机听魏如萱的《鱼》，把声音开到合适的状态，耳朵被音乐灌满，一丝风都挤不进来。</p>
<p>也许是解决了一个长期困扰的问题，也许是这首歌的曲调感染了我，也许是WAWA演绎这首歌传递出来的决然打动了我，一路上我的嘴角一直微微上扬。</p>
<p>感觉自行车骑得特别轻松，很久都没有感受到这么自由的味道了。</p>
<p>.</p>
<h1 id="2023-x2F-2-x2F-22"><a href="#2023-x2F-2-x2F-22" class="headerlink" title="2023&#x2F;2&#x2F;22"></a>2023&#x2F;2&#x2F;22</h1><p>今早尝试配置gosc挑选的项目ignite时突然意识到一个问题：pip install时需要把代理关掉。</p>
<p>我之前还觉得直接pip install不够快把原因放在镜像源上，代理也没关，那么之前遇到的安装问题会不会部分与这个有关呢…</p>
<p>——————————</p>
<p>上午线性回归分析上得我好感动，好喜欢周老师！喜欢她的讲课方式以及她的亲和力，尽管确实语速和进度有点点快。</p>
<p>上课的时候涉及到部分概率论知识我发现我必须思考半天才能反应过来呜呜呜，感觉自己好菜。</p>
<p>希望这门课能坚持下去，不太想中途退课了。</p>
<p>不过有点担心没有修统推就来上这门课会不会跟不上，那就浅浅立个flag: 有问题多问老师，我也很想和周老师建立strong connection：)</p>
<p>——————————</p>
<p>今天使用了了<code>srun --pty -G1 bash</code>先分配了一块GPU，如下图：</p>
<p><img src="/2023/02/19/moyu4/3.jpg"></p>
<p>注意：不能再开一个终端运行程序，这样相当于还是在无GPU环境下运行。应该等第二行命令出来后继续在这个终端输入（即此时提示已经被分配好了GPU），可以在右侧栏里看到本终端显示的不是bash而是srun。</p>
<p>此外被分配好GPU后似乎会自动退出虚拟环境，需要再次进入。</p>
<p>果然，昨天的报错解决了。</p>
<div class="note info modern"><p>昨天的报错：</p>
<p>RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver</p>
</div>

<p>与昨天没分配GPU的报错相比，这次终端的界面为：</p>
<p><img src="/2023/02/19/moyu4/2.jpg"></p>
<p>最下面一排的loss、ppl等的值貌似会随着进程的推进而改变。</p>
<p>……</p>
<p>突然理解“学堂路车神”了，比如我用srun命令跑测试，终端断开则任务终止，而我暂时还不知道使它在我电脑熄屏的状态下还能跑下去的办法……</p>
<p>搜到了一篇解决方法：<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/PcLjtOM9CfXmvDWidqAGww">如何优雅地在学堂路上骑着车跑代码</a></p>
<p>————————————</p>
<p>关于困惑度ppl的理解：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/hxxjxw/article/details/107722646">困惑度PPL (perplexity)</a></p>
<p>关于学习率的理解：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/mxh3600/article/details/124261609">学习率的理解</a></p>
<p>————————————</p>
<p>任务1的代码已经跑了快四个小时了，现在正在跑epoch 004，简单记录一下前三次的结果：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">023-02-22 06:51:09 | INFO | valid | epoch 001 | valid on <span class="string">&#x27;valid&#x27;</span> subset | loss 6.667 | ppl 101.61 | wps 79019.2 | wpb 2034.1 | bsz 4 | num_updates 3147</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">023-02-22 07:50:30 | INFO | valid | epoch 002 | valid on <span class="string">&#x27;valid&#x27;</span> subset | loss 5.783 | ppl 55.08 | wps 78428.3 | wpb 2034.1 | bsz 4 | num_updates 6295 | best_loss 5.783</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">023-02-22 08:49:53 | INFO | valid | epoch 003 | valid on <span class="string">&#x27;valid&#x27;</span> subset | loss 5.496 | ppl 45.14 | wps 78575.5 | wpb 2034.1 | bsz 4 | num_updates 9443 | best_loss 5.496</span><br></pre></td></tr></table></figure>

<p>可以看到ppl是越来越小了</p>
<p>————————————</p>
<p>emmm我以为跑5个epoch就可以了 ，不过为啥程序还会继续跑（？</p>
<p>看来我还需要恶补一下相关知识（悲</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">023-02-22 09:49:14 | INFO | valid | epoch 004 | valid on <span class="string">&#x27;valid&#x27;</span> subset | loss 5.357 | ppl 40.99 | wps 78507.8 | wpb 2034.1 | bsz 4 | num_updates 12591 | best_loss 5.357</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">023-02-22 10:48:31 | INFO | valid | epoch 005 | valid on <span class="string">&#x27;valid&#x27;</span> subset | loss 5.265 | ppl 38.44 | wps 79030.8 | wpb 2034.1 | bsz 4 | num_updates 15739 | best_loss 5.265</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">023-02-22 11:47:44 | INFO | valid | epoch 006 | valid on <span class="string">&#x27;valid&#x27;</span> subset | loss 5.205 | ppl 36.89 | wps 78922.2 | wpb 2034.1 | bsz 4 | num_updates 18887 | best_loss 5.205</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">023-02-22 12:47:07 | INFO | valid | epoch 007 | valid on <span class="string">&#x27;valid&#x27;</span> subset | loss 5.172 | ppl 36.05 | wps 79688.5 | wpb 2034.1 | bsz 4 | num_updates 22035 | best_loss 5.172</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">023-02-22 13:46:24 | INFO | valid | epoch 008 | valid on <span class="string">&#x27;valid&#x27;</span> subset | loss 5.144 | ppl 35.36 | wps 78489.3 | wpb 2034.1 | bsz 4 | num_updates 25184 | best_loss 5.144</span><br></pre></td></tr></table></figure>

<p>百度了一下epoch的值为多少比较合适，说50~200，一般100就够了……</p>
<p>这才到第9次，不如就等它跑吧……我先理解一下fairseq框架再复习一下transformers模型后直接开始别的任务吧……</p>
<p>————————————</p>
<p>发现图书馆真是一个睡觉的好地方，又暖和又安静。感觉以后中午都可以去图书馆睡觉了，小憩十五分钟人都精神一点。</p>
<p>空隙时间写线性回归作业时感觉好难，每道题都不太会写，好不容易会写的一道题结果因为漏写y&#x3D;和x&#x3D;被雨课堂判错了，sad。</p>
<p>这才上了三天的课，还是轻量级别的。感觉不想冲突选大物了，也不想选英语，唉，不知道该怎么办，再考虑考虑吧。</p>
<p>.</p>
<h1 id="2023-x2F-2-x2F-23"><a href="#2023-x2F-2-x2F-23" class="headerlink" title="2023&#x2F;2&#x2F;23"></a>2023&#x2F;2&#x2F;23</h1><div class="note info modern"><p>月照西时多一横，清风共我扫阶尘。</p>
</div>

<p>啧，仅仅过了一天我就决定把统辅的两门课退掉了，然后把听说和大物修回来。</p>
<p>也许在我心里，还是对降转抱有一丝希望吧。</p>
<p>没办法，尽管我很喜欢周老师和邓老师，可是一是我上学期没有修统推，二是如果硬把这两门课上下来的话我可能心态会崩并且就彻底没时间搞科研了。</p>
<p>说到科研我就气，昨天模型跑的好好的，好不容易可以跑训练了我还很高兴，结果官方文档上的evaluate代码用不了，抱了一个莫名其妙的错，有点难过沮丧。为什么别人做入门任务看上去就那么轻松啊😔</p>
<p>下午尝试着写物理实验报告，也没有写完，后面的等督写完了直接借鉴他的吧orz，我是真的不喜欢做实验写报告啊。</p>
<p>——————————</p>
<p>呜呜呜时薪300的家教没了，学生为了和学校进度一致和同学组了个班。算了算了，正好有更多地时间干别的事了。</p>
<p>不想上学了🥀想把课都退掉。</p>
<p>.</p>
<h1 id="2023-x2F-2-x2F-24"><a href="#2023-x2F-2-x2F-24" class="headerlink" title="2023&#x2F;2&#x2F;24"></a>2023&#x2F;2&#x2F;24</h1><p>[FastCorrect&amp;Fairseq学习笔记](<a target="_blank" rel="noopener" href="https://www.cnblogs.com/hanlaomo/p/16708102.html">FastCorrect&amp;Fairseq学习笔记 - MyTD21 - 博客园 (cnblogs.com)</a>)：解释了train的相关命令</p>
<p>发现了fairseq&#x2F;examples&#x2F;language_model官方教程中的一个小bug，而这个bug让我昨天跑evaluate命令报错了😡，bug&amp;solution见<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/fairseq/issues/4992">github issues 4992</a></p>
<p>解决这个问题后模型就能够被评测了，开心</p>
<p>接下来尝试改变学习率（即–lr的值）来进行微调，不过还需要查一些资料看看怎么在已经经过预训练的模型上微调。</p>
<p>——————————</p>
<p>🥀🥀🥀🥀</p>
<p>材料力学和工热一样无聊，多元统计和线性回归有点跟不上，今天头疼的要命，尤其是被北京的妖风一吹。花了一个晚上写BID项目的报名表，我也不知道我报这个干什么，甚至不知道能不能被选上，我只是觉得它的宣传链接写的特别吸引人，再加上我该死的好奇心，所以想报名试试。</p>
<p>感觉第一周过的好糟糕，以往刚开学的时候心态都是很稳定的，但这次不知道为什么很焦虑😞</p>
<p>第一周唯一令人开心的事就是终于配好fairseq环境并且训练模型了，也解决了评测的bug完成了第一次评测。或许这就是科研的魅力吧，<strong>给你绝望却又不吝啬地留你一丝希望</strong>。</p>
<p>其实如果有可能，我完全可以把材料力学和流体力学退掉而不是退掉多元统计和线性回归，但是我不敢冒这么大的风险。</p>
<p>没关系，能活着毕业就已经很不容易了。</p>
<p>.</p>
<h1 id="2023-x2F-2-x2F-25"><a href="#2023-x2F-2-x2F-25" class="headerlink" title="2023&#x2F;2&#x2F;25"></a>2023&#x2F;2&#x2F;25</h1><p>周末宜睡懒觉。早上出门前涂了个口红，大概是豆沙色的？反正感觉很柔和，和之前老妈送我的纪梵希的番茄色口红不一样，就算不化妆也一点都不违和（很适合我这种懒人）。配上今天的发型（其实就是正常的披发然后加了一个发卡），倒有点像郑爽在《微微一笑很倾城》里的一个造型。我特别喜欢这部电视剧，记得当时是和雪雪一起看的，会笑死。不长，但很好看，很甜，其实我一直都很吃郑爽的颜，想想这部剧距今也有7年了啊，好多事都发生了改变。</p>
<p>一大早心情还不错，希望下午的OOP小教员考试不至于一道题都写不出来，当时报名的时候也和以前一样，想的是万一呢😂但是一道题都AC不了有点丢人，考完了正好去和老杜一起吃饭嘻嘻，感觉大学就应该这样。</p>
<p>——————————</p>
<p>好吧，果然还是太自不量力了，一道编程题都没写出来，哪怕延了半小时。主要是好多有关类的知识要么忘记了，要么还没学过，再加上以前没写过这样的OJ题目，有点无从下手，遂摆了。</p>
<p>晚上和老杜一起吃了饭后去五道口逛了逛商场，感觉没什么好玩的，倒是骑自行车的时候和老杜聊天挺有趣的。</p>
<p>周末还是应该放松点（好吧，其实是我一点也提不起学习的兴趣😞</p>
<p>明天to-do list：</p>
<ul>
<li>流力作业剩下的题目</li>
<li>物理实验报告</li>
<li>材力实验预习</li>
<li>改变任务一的学习率继续训练模型</li>
</ul>
<p>说实话，我更想退流力和材力，可惜我还没有那个勇气。</p>
<p>——————————</p>
<p>记录一下目前报名了的：</p>
<ul>
<li>BID项目（有那么一点点对商业的兴趣，希望能过</li>
<li>Google STEP（报着玩玩，估计过不了</li>
<li>Optiver IT data scientist internship（这个也是，估计过不了</li>
</ul>
<p>明天再看看有什么心仪的公司，感觉我报的实习bar都好高😔一看就是我过不了的那种，也就只能投着玩玩了，等三四月份再投一些小公司试试吧，希望有公司要我呜呜呜。</p>
<p>.</p>
<h1 id="2023-x2F-2-x2F-26"><a href="#2023-x2F-2-x2F-26" class="headerlink" title="2023&#x2F;2&#x2F;26"></a>2023&#x2F;2&#x2F;26</h1><p>目前遇到的问题是在模型被训练后想进行微调，但是具体的操作不知道。看transformers的官方网站以及进行搜索后还是很懵，如果按照任务一的官方文档中的代码，我不确定微调是不是继续用fairseq train的代码，如果继续使用的话需要改变checkpoints储存的位置吗？如果不继续用的话那应该用什么命令进行微调呢？</p>
<p>我打算仍用fairseq train的命令，然后把–lr改为0.001，同时设置–max-epoch 30，此外把保存模型的文件夹换成checkpoints&#x2F;transformer_wikitext-103-1。这样应该不算微调算重新训练（？），不确定，反正如果算重新训练的话似乎也不造成什么影响吧…</p>
<p>先挂着，让模型自己跑吧orz</p>
<p>————————————</p>
<p>今天又看了一部好小说，喜欢这种娓娓道来的笔法。不知为何，竟看的又红了眼眶。</p>
<div class="note info modern"><p>有人一生都在爱，今日或许爱这一个，明日又换个旁的。</p>
<p>可有些人，一生只能爱一人。</p>
<p>这不好，十分不好，可是也没法子。</p>
<p>一朝一暮是一日，朝朝暮暮就是一生。</p>
<p>只要有片刻，哪怕只有片刻，你所想所念哪怕有片刻能实现，这一生也便不算白活。</p>
</div>

<p>————————————</p>
<p>听《断桥残雪》，翻评论，也想模仿高赞评论一句：</p>
<p>当年碰巧去了一趟西湖，看了三潭印月雷峰塔和断桥，当时还轻轻哼唱着《断桥残雪》</p>
<p>断桥全是人，没有雪</p>
<p>七月怎么会下雪呢</p>
<p>————————————</p>
<p>昨天的To-do list：</p>
<ul>
<li>流力作业剩下的题目   【√】</li>
<li>物理实验报告</li>
<li>材力实验预习   【√】</li>
<li>改变任务一的学习率继续训练模型   【√】</li>
</ul>
<p>这一周的进度，感觉都赶得上之前好几周的进度了（因为之前要么是在外实践&amp;旅游，要么是配环境配的生不如死…</p>
<p>虽然也不是很快，因为前两个任务的代码都写好了，唯一的工作量就是理解下代码的意识然后改改超参数而已（不过对于小白来说还是有点困难）</p>
<p>比不过别人，只能按照自己的节奏来了。</p>
<p>想想目前我的绩点也就3.6+，估计转去了CS得排60%或者70%开外了🙃</p>
<p>幸好我的目的从来都不是保研，虽然我的理想比保研难得多。</p>
<p>.</p>
<p>今日mark的几篇文章：</p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/miraclepbc/p/15887807.html">微调预训练模型</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_35091353/article/details/117322293">pytorch之warm-up预热学习策略_还能坚持的博客-CSDN博客_pytorch warmup</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/gnnucv/p/16064423.html">深度学习训练过程中的学习率衰减策略及pytorch实现 - gnnu_cv - 博客园 (cnblogs.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://developer.habana.ai/tutorials/pytorch/distilbert-sequence-classification-with-imdb-reviews/">DistilBert Sequence Classification with IMDb Reviews - Habana Developers</a></p>
<p>.</p>
<p>——————————</p>
<p>麻了，之前在教室一直连不上服务器，把电脑关机后回来开机后重新连，倒是连上了，但是不知道为什么找不到&#x2F;data_new&#x2F;private&#x2F;liuyadi这个文件…</p>
<p>&#x2F;data_new&#x2F;private里居然只显示两个人的文件……</p>
<p>而且感觉&#x2F;home&#x2F;liuyadi里也像是刚搭建服务器一样……什么都没有</p>
<p>🥀🥀🥀🥀🥀🥀🥀</p>
<p>为什么要这样对我😭好不容易快跑完两个模型了，如果真的清零了那我会很难受很崩溃的🥀</p>
<p>明天再看看能不能恢复🥀不行的话问问学长是怎么回事吧😭😭😭😭</p>
<h1 id="2023-x2F-2-x2F-27"><a href="#2023-x2F-2-x2F-27" class="headerlink" title="2023&#x2F;2&#x2F;27"></a>2023&#x2F;2&#x2F;27</h1><p>今早一看发现远程服务器已经恢复正常了，怪，难道每周日它就会把上面占用GPU的任务强行停止吗？</p>
<p>头脑一热抢了周杰伦呼和浩特演唱会的票，抢票过程感觉异常丝滑，我还以为要卡很久都不一定抢到，没想到几秒钟就完成付款了，可能是看台比较好抢的原因，嘿嘿。</p>
<p>对于周杰伦来说，看台足以，对于许嵩和华晨宇来说，我更希望抢到最高价位的票，尤其是许嵩。</p>
<p>感觉钱包不保😥</p>
<p>今天的科研进度：任务2再跑三个学习率应该就差不多了，任务1还在跑，等明天试试看能不能用fairseq-hydra-train函数</p>
<h1 id="2023-x2F-2-x2F-28"><a href="#2023-x2F-2-x2F-28" class="headerlink" title="2023&#x2F;2&#x2F;28"></a>2023&#x2F;2&#x2F;28</h1><p>把任务2所需要的实验数据都跑完了，只剩报告了。任务1使用fairseq-hydra-train函数失败，故不管了，跑了的几组数据居然第一次的最好，ppl最低，就这样吧，直接用这些数据写实验报告好了。</p>
<p>明天可以开始任务3了。</p>
<p>————————————</p>
<p>华华的演唱会官宣了😭第一场是4月9号10号在杭州，240min+，我哭死。</p>
<p>火星演唱会什么都好，就是难得抢票，而且我看最贵的票价也只有1280（可能因为和21年的形式差不多也没有座位？），呜呜呜希望我能抢到票，今年还是巡回，决定先抢一张看台，再在别的场次试试抢一张内场。如果能开到北京就好了，或者离我近一点就好了，21年的演唱会明明我可以去的，但是在海口，离北京太远了🥀</p>
<p>真好，今年有机会去看他了。</p>
<h1 id="2023-x2F-3-x2F-1"><a href="#2023-x2F-3-x2F-1" class="headerlink" title="2023&#x2F;3&#x2F;1"></a>2023&#x2F;3&#x2F;1</h1><div class="note info modern"><p>爱在当下。</p>
</div>

<p>三月！期待春天以及春天发生的事情。</p>
<p>决定利用因退课多出来的时间多学点感兴趣的东西，可惜我总喜欢浪费时间在看小说上面🥀</p>
<p>从numpy和pandas开始吧，毕竟总是听到或看到却不知道怎么用。其次功利点来说，好多实习都要求掌握这些。</p>
<p>————————————</p>
<p>有一种晚上和早上不是同一天的想法（</p>
<p>平等地羡慕每一个在清华如鱼得水的人，我觉得我能活着毕业就已经很不错了</p>
<p>什么时候才能离开这里啊（不是被退学）</p>
<h1 id="2023-x2F-3-x2F-2"><a href="#2023-x2F-3-x2F-2" class="headerlink" title="2023&#x2F;3&#x2F;2"></a>2023&#x2F;3&#x2F;2</h1><p>zz这几天看小说居然倾向于看火葬场剧情了，也开始接受虐文了，尽管心里会很难受，彷佛被针扎了一样疼。看文的很多时候都想感叹，<strong>人生若只如初见</strong>啊。当年嫌这句诗写得过于直白浅显，却忽略了其中包含的“却道天凉好个秋”的无限心酸与无奈。</p>
<p>人生若只如初见，何事秋风悲画扇。</p>
<p>————————————</p>
<p>今天看到了棱镜的巡演，在天津有两场，于是就去约gbj了😁希望能抢到票</p>
<p>音乐让未来充满了色彩。</p>
<p>————————————</p>
<p>今日任务及完成情况：</p>
<p>1.材料力学实验报告√    大致完成了，还差小结和结论（如果有的话<br>2.物理实验预习√<br>3.流体力学静力学部分复习巩固<br>4.材料力学3.1日内容复习巩固<br>5.任务3推进，最好到微调之前√（到3.2训练了<br>6.英语听说U2题目<br>7.numpy学习“形状操纵”√</p>
<p>3和4就留到明天早上吧。</p>
<p>.</p>
<h1 id="2023-x2F-3-x2F-3"><a href="#2023-x2F-3-x2F-3" class="headerlink" title="2023&#x2F;3&#x2F;3"></a>2023&#x2F;3&#x2F;3</h1><p>怎么会有课一周两次其中有一次是只有单周上的啊，白跑了一遍教室，我是说怎么到了一点十分教室里都没有人呢。</p>
<p>昨天收到鸿雁计划的消息，居然两个都被接受了，结果我开始犹豫去不去上课了，如果去的话周末就没有了呜呜呜。还有就是接到了BID计划申请书筛选通过的通知，周日上午面试，还好不是周六，不过就算是周六的话似乎也可以面完后出去玩。</p>
<p>无所谓吧，我现在尊重我每一次头脑发热的决定😂</p>
<p>——————————————</p>
<p>SOS，突然满脑子都是昨天看的那篇虐文其中一个美好到极致的情节，作者也写道：那是一个让后来的他们想到都会落泪的一个场景😭</p>
<p>真可惜啊。</p>
<p>——————————————</p>
<p>今天打了本学期的第一次盘，感觉好几个月没打盘除了体力有点跟不上外扔盘的手感比我想象的好得多，看来一学期的飞盘体育课还是有效的，女子single比赛真的能促进人的进步。</p>
<p>明天又和老杜一起约饭，然后顺便出去逛逛，嘿嘿。这才叫大学生活嘛。</p>
<h1 id="2023-x2F-3-x2F-4"><a href="#2023-x2F-3-x2F-4" class="headerlink" title="2023&#x2F;3&#x2F;4"></a>2023&#x2F;3&#x2F;4</h1><p>今天吃的那家韩餐明显比上次那家好吃，然后和dyj去逛了国贸，回来整个人累得不行。几乎是一出地铁站，就感受到了国贸那一片CBD的气质，给人强烈的距离感，觉得自己和那些是不属于同一世界的。</p>
<p>不过，终于感受到了大学生活的实体感。</p>
<p>晚上小说暂时看够了后写完了剩下的流体力学作业，不知为何，总觉得英文原版的书籍对应的作业题都简单一点，比如线代，比如概率论等等。</p>
<p>今天的小小愿望：明天的面试顺利（至少不要让我十分尴尬</p>
<h1 id="2023-x2F-3-x2F-5"><a href="#2023-x2F-3-x2F-5" class="headerlink" title="2023&#x2F;3&#x2F;5"></a>2023&#x2F;3&#x2F;5</h1><p>今早的面试感觉聊的挺舒服，让我感到惊讶的是居然只有一个学长和一个学姐来面试，我还以为会有很多人呢。面试最大的难点在于面试地点不好找（</p>
<p>感觉就是我过去的经历以及我在网上看的一些杂七杂八的东西可以支撑我在那<del>东扯西扯</del>大致游刃有余地回答😂</p>
<p>不管能不能过，感觉好久没和人这么开心地聊过天了，尤其是那位学长，感觉气场没有那位学姐强😂似乎博士快毕业了，对如今本科生的卷度还没有清晰地认知（）</p>
<p>服务器似乎一到周日晚上就用不了了，等周一上午才能恢复正常。再花个一两天把任务3给跑出来，然后开始任务4吧。希望能尽快完成，唉，似乎只有我还没有完成入门任务了吧，其他人甚至都已经不填那个入门任务的进度文档了😥</p>
<p>明天又是新的一周了，祝我好运。</p>
<h1 id="2023-x2F-3-x2F-6"><a href="#2023-x2F-3-x2F-6" class="headerlink" title="2023&#x2F;3&#x2F;6"></a>2023&#x2F;3&#x2F;6</h1><div class="note info modern"><p>人生不是轨道，是旷野。</p>
</div>

<p>决定如果这次转系成功了就以这个为标题写一篇心路历程。</p>
<p>————————————</p>
<p>晚上系飞盘队为我们过女生节，稍稍改了一下规则，使得女生的参与度大大提升，虽然很累，但还挺有趣的。</p>
<p>晚上把电子电路作业和大物作业都写了，感觉现在写大物明显感到熟练的多，不知道是因为学完了理论力学的原因还是因为之前写过一遍的原因，希望这次能有个好结果吧。</p>
<p>任务3跑了3组实验，就这样吧，有一个我认为数据很好的，也懒得管了。但愿我能快点上手任务4，总对自己动手写代码这件事有点恐惧心理，唉。</p>
<h1 id="2023-x2F-3-x2F-7"><a href="#2023-x2F-3-x2F-7" class="headerlink" title="2023&#x2F;3&#x2F;7"></a>2023&#x2F;3&#x2F;7</h1><p>呜呜呜收到了BID项目的拒信，尝试涉猎商业知识失败。</p>
<p>每次我觉得自己和面试官聊的很好很愉快的时候结果总是与我预期的相反，或许只是我感觉比较愉快（）</p>
<p>今天尝试写了一下OOP作业，好难啊（sigh，第一次作业的难度就有点超出我的想象，果然是贵系</p>
<p>没办法，只能好好学呗，到时候如果转不成系的话也只能好好学而已，如果这次转不成，那就直接准备本科就业了（）</p>
<p>感觉出国的话如果算上本专业的绩点是毫无优势的，何况还得跨申……</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://liuydd.github.io">Sheeta Liu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://liuydd.github.io/2023/02/19/moyu4/">http://liuydd.github.io/2023/02/19/moyu4/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://liuydd.github.io" target="_blank">等到天亮我们都寻找到最漂亮的愿望</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="../../../../tags/diary/">diary</a></div><div class="post_share"><div class="social-share" data-image="../../../../images/cover33.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="../../13/yuanlin/"><img class="next-cover" src="../../../../images/cover32.JPG" onerror="onerror=null;src='../../../../img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">游园惊梦·聊赠一枝春</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="../../../01/08/moyu3/" title="Sheetaの摸鱼日记（三）"><img class="cover" src="../../../../images/cover31.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-08</div><div class="title">Sheetaの摸鱼日记（三）</div></div></a></div><div><a href="../../../../2022/09/12/moyu2/" title="Sheetaの摸鱼日记（二）"><img class="cover" src="../../../../images/cover14.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-12</div><div class="title">Sheetaの摸鱼日记（二）</div></div></a></div><div><a href="../../../../2022/07/31/moyu/" title="Sheetaの摸鱼日记"><img class="cover" src="../../../../images/cover11.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-07-31</div><div class="title">Sheetaの摸鱼日记</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="../img/touxiang2.jpg" onerror="this.onerror=null;this.src='../img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Sheeta Liu</div><div class="author-info__description">生活不需要意义，吃点好的，我也爱你</div></div><div class="card-info-data site-data is-center"><a href="../archives/"><div class="headline">文章</div><div class="length-num">18</div></a><a href="../tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/"><div class="headline">分类</div><div class="length-num">8</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/liuydd"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">众生皆苦，唯有自渡...</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2023-x2F-2-x2F-20"><span class="toc-number">1.</span> <span class="toc-text">2023&#x2F;2&#x2F;20</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2023-x2F-2-x2F-21"><span class="toc-number">2.</span> <span class="toc-text">2023&#x2F;2&#x2F;21</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2023-x2F-2-x2F-22"><span class="toc-number">3.</span> <span class="toc-text">2023&#x2F;2&#x2F;22</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2023-x2F-2-x2F-23"><span class="toc-number">4.</span> <span class="toc-text">2023&#x2F;2&#x2F;23</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2023-x2F-2-x2F-24"><span class="toc-number">5.</span> <span class="toc-text">2023&#x2F;2&#x2F;24</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2023-x2F-2-x2F-25"><span class="toc-number">6.</span> <span class="toc-text">2023&#x2F;2&#x2F;25</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2023-x2F-2-x2F-26"><span class="toc-number">7.</span> <span class="toc-text">2023&#x2F;2&#x2F;26</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2023-x2F-2-x2F-27"><span class="toc-number">8.</span> <span class="toc-text">2023&#x2F;2&#x2F;27</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2023-x2F-2-x2F-28"><span class="toc-number">9.</span> <span class="toc-text">2023&#x2F;2&#x2F;28</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2023-x2F-3-x2F-1"><span class="toc-number">10.</span> <span class="toc-text">2023&#x2F;3&#x2F;1</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2023-x2F-3-x2F-2"><span class="toc-number">11.</span> <span class="toc-text">2023&#x2F;3&#x2F;2</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2023-x2F-3-x2F-3"><span class="toc-number">12.</span> <span class="toc-text">2023&#x2F;3&#x2F;3</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2023-x2F-3-x2F-4"><span class="toc-number">13.</span> <span class="toc-text">2023&#x2F;3&#x2F;4</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2023-x2F-3-x2F-5"><span class="toc-number">14.</span> <span class="toc-text">2023&#x2F;3&#x2F;5</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2023-x2F-3-x2F-6"><span class="toc-number">15.</span> <span class="toc-text">2023&#x2F;3&#x2F;6</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2023-x2F-3-x2F-7"><span class="toc-number">16.</span> <span class="toc-text">2023&#x2F;3&#x2F;7</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="../2023/02/19/moyu4/" title="Sheetaの摸鱼日记（四）"><img src="../images/cover33.jpg" onerror="this.onerror=null;this.src='../img/404.jpg'" alt="Sheetaの摸鱼日记（四）"/></a><div class="content"><a class="title" href="../2023/02/19/moyu4/" title="Sheetaの摸鱼日记（四）">Sheetaの摸鱼日记（四）</a><time datetime="2023-02-19T14:09:03.000Z" title="发表于 2023-02-19 22:09:03">2023-02-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="../2023/02/13/yuanlin/" title="游园惊梦·聊赠一枝春"><img src="../images/cover32.JPG" onerror="this.onerror=null;this.src='../img/404.jpg'" alt="游园惊梦·聊赠一枝春"/></a><div class="content"><a class="title" href="../2023/02/13/yuanlin/" title="游园惊梦·聊赠一枝春">游园惊梦·聊赠一枝春</a><time datetime="2023-02-13T15:40:56.000Z" title="发表于 2023-02-13 23:40:56">2023-02-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="../2023/01/14/paper/" title="NLP前沿论文"><img src="../images/cover28.jpg" onerror="this.onerror=null;this.src='../img/404.jpg'" alt="NLP前沿论文"/></a><div class="content"><a class="title" href="../2023/01/14/paper/" title="NLP前沿论文">NLP前沿论文</a><time datetime="2023-01-14T09:19:31.000Z" title="发表于 2023-01-14 17:19:31">2023-01-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="../2023/01/08/moyu3/" title="Sheetaの摸鱼日记（三）"><img src="../images/cover31.jpg" onerror="this.onerror=null;this.src='../img/404.jpg'" alt="Sheetaの摸鱼日记（三）"/></a><div class="content"><a class="title" href="../2023/01/08/moyu3/" title="Sheetaの摸鱼日记（三）">Sheetaの摸鱼日记（三）</a><time datetime="2023-01-08T12:42:47.000Z" title="发表于 2023-01-08 20:42:47">2023-01-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="../2022/12/14/transformers/" title="Transformers🤗"><img src="../images/cover29.jpg" onerror="this.onerror=null;this.src='../img/404.jpg'" alt="Transformers🤗"/></a><div class="content"><a class="title" href="../2022/12/14/transformers/" title="Transformers🤗">Transformers🤗</a><time datetime="2022-12-14T06:20:14.000Z" title="发表于 2022-12-14 14:20:14">2022-12-14</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By Sheeta Liu</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div><canvas class="fireworks" style="position: fixed; left: 0; top: 0; z-index: 1; pointer-events: none;" ></canvas>
<script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script>
<script type="text/javascript" src="/js/fireworks.js"></script>
<% } %></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="../../../../js/utils.js"></script><script src="../../../../js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="../../../../js/search/local-search.js"></script><div class="js-pjax"><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://twikoo-lake-alpha-67.vercel.app',
      region: '',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://twikoo-lake-alpha-67.vercel.app',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/live2dw/assets/tororo.model.json"},"display":{"superSample":2,"position":"left","width":150,"height":300,"hOffset":20,"vOffset":-90},"mobile":{"show":false,"scale":1},"react":{"opacityDefault":0.3,"opacityOnHover":0.3,"opacity":0.95},"log":false});</script></body></html>